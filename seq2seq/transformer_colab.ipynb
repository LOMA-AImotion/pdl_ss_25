{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a64cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "!pip install lightning\n",
    "import lightning as L\n",
    "\n",
    "class PositionEncoding(nn.Module):\n",
    "    def __init__(self, d_model=2, max_len=6):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(start=0, end=max_len, step=1).float().unsqueeze(1)\n",
    "        embedding_index = torch.arange(start=0, end=d_model, step=2).float()\n",
    "        div_term = 1/torch.tensor(10000.0)**(embedding_index / d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, word_embeddings):\n",
    "        return word_embeddings + self.pe[:word_embeddings.size(0), :]\n",
    "\n",
    "class Attention(nn.Module): \n",
    "    def __init__(self, d_model=2):\n",
    "        super().__init__()\n",
    "        self.d_model=d_model\n",
    "        self.W_q = nn.Linear(in_features=d_model, out_features=d_model)\n",
    "        self.W_k = nn.Linear(in_features=d_model, out_features=d_model)\n",
    "        self.W_v = nn.Linear(in_features=d_model, out_features=d_model)\n",
    "        self.row_dim = 0\n",
    "        self.col_dim = 1\n",
    "    def forward(self, encodings_for_q, encodings_for_k, encodings_for_v, mask=None):\n",
    "        q = self.W_q(encodings_for_q)\n",
    "        k = self.W_k(encodings_for_k)\n",
    "        v = self.W_v(encodings_for_v)\n",
    "        sims = torch.matmul(q, k.T)\n",
    "        scaled_sims = sims / torch.sqrt(torch.tensor(self.d_model))\n",
    "        if mask is not None:\n",
    "            scaled_sims = scaled_sims.masked_fill(mask=mask, value=-1e9)\n",
    "        attention_percents = F.softmax(scaled_sims, dim=self.col_dim)\n",
    "        attention_scores = torch.matmul(attention_percents, v)\n",
    "        return attention_scores\n",
    "\n",
    "class DecoderOnlyTransformer(L.LightningModule):\n",
    "    def __init__(self, num_tokens=4, d_model=2, max_len=6, seed=42):\n",
    "        super().__init__()\n",
    "        L.seed_everything(seed=seed)\n",
    "        self.we = nn.Embedding(num_embeddings=num_tokens, embedding_dim=d_model)     \n",
    "        self.pe = PositionEncoding(d_model=d_model, max_len=max_len)\n",
    "        self.self_attention = Attention(d_model=d_model)\n",
    "        self.fc_change_direction = nn.Linear(in_features=d_model, out_features=d_model)\n",
    "        self.fc_output = nn.Linear(in_features=d_model, out_features=num_tokens)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        word_embeddings = self.we(token_ids)        \n",
    "        inputs_position_encoded = self.pe(word_embeddings)\n",
    "        mask = torch.tril(torch.ones((token_ids.size(dim=0), token_ids.size(dim=0)), device=self.device))\n",
    "        mask = mask == 0\n",
    "        self_attention_values = self.self_attention(inputs_position_encoded, \n",
    "                                                    inputs_position_encoded, \n",
    "                                                    inputs_position_encoded, \n",
    "                                                    mask=mask)\n",
    "        \n",
    "        residual_connection_values = inputs_position_encoded + self_attention_values\n",
    "        change_directions = self.fc_change_direction(residual_connection_values)\n",
    "        residual_connection_values = residual_connection_values + change_directions\n",
    "\n",
    "        fc_layer_output = self.fc_output(residual_connection_values)\n",
    "        return fc_layer_output\n",
    "    \n",
    "    def configure_optimizers(self): \n",
    "        return Adam(self.parameters(), lr=0.1)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx): \n",
    "        input_tokens, labels = batch\n",
    "        output = self.forward(input_tokens[0])\n",
    "        loss = self.loss(output, labels[0])\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab473cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 43\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\Schiendorfer\\Anaconda3\\envs\\llm\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "\n",
      "  | Name                | Type             | Params | Mode \n",
      "-----------------------------------------------------------------\n",
      "0 | we                  | Embedding        | 10     | train\n",
      "1 | pe                  | PositionEncoding | 0      | train\n",
      "2 | self_attention      | Attention        | 18     | train\n",
      "3 | fc_change_direction | Linear           | 6      | train\n",
      "4 | fc_output           | Linear           | 15     | train\n",
      "5 | loss                | CrossEntropyLoss | 0      | train\n",
      "-----------------------------------------------------------------\n",
      "49        Trainable params\n",
      "0         Non-trainable params\n",
      "49        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "9         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "c:\\Users\\Schiendorfer\\Anaconda3\\envs\\llm\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\Schiendorfer\\Anaconda3\\envs\\llm\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:310: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** BEFORE TRAINING ***\n",
      ">>>  what is DL <EOS>\n",
      "Predicted Tokens:\n",
      "\n",
      "tensor([2, 1, 4])\n",
      "\t DL\n",
      "\t is\n",
      "\t <EOS>\n",
      "*** RUNNING TRAINING ***\n",
      "Epoch 29: 100%|██████████| 2/2 [00:00<00:00, 135.84it/s, v_num=2]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 2/2 [00:00<00:00, 101.40it/s, v_num=2]\n",
      "*** AFTER TRAINING ***\n",
      ">>>  what is DL <EOS>\n",
      "Predicted Tokens:\n",
      "\n",
      "tensor([3, 4])\n",
      "\t awesome\n",
      "\t <EOS>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "!pip install lightning\n",
    "import lightning as L\n",
    "\n",
    "# inspired by https://github.com/StatQuest/decoder_transformer_from_scratch\n",
    "\n",
    "token_to_id = {'what' : 0,\n",
    "               'is' : 1,\n",
    "               'DL' : 2,\n",
    "               'awesome': 3,\n",
    "               '<EOS>' : 4,\n",
    "              }\n",
    "\n",
    "id_to_token = {v: k for k, v in token_to_id.items()}\n",
    "\n",
    "sentence1 = [token_to_id[\"what\"],\n",
    "            token_to_id[\"is\"], \n",
    "            token_to_id[\"DL\"], \n",
    "            token_to_id[\"<EOS>\"],\n",
    "            token_to_id[\"awesome\"]]\n",
    "\n",
    "sentence2 = [token_to_id[\"DL\"],\n",
    "            token_to_id[\"is\"], \n",
    "            token_to_id[\"what\"], \n",
    "            token_to_id[\"<EOS>\"], \n",
    "            token_to_id[\"awesome\"]]\n",
    "\n",
    "inputs = torch.tensor([sentence1, sentence2])\n",
    "\n",
    "label1 = [token_to_id[\"is\"], \n",
    "        token_to_id[\"DL\"], \n",
    "        token_to_id[\"<EOS>\"], \n",
    "        token_to_id[\"awesome\"], \n",
    "        token_to_id[\"<EOS>\"]]\n",
    "\n",
    "label2 = [token_to_id[\"is\"], \n",
    "        token_to_id[\"what\"], \n",
    "        token_to_id[\"<EOS>\"], \n",
    "        token_to_id[\"awesome\"], \n",
    "        token_to_id[\"<EOS>\"]]\n",
    "\n",
    "labels = torch.tensor([label1,  \n",
    "                       label2\n",
    "                       ])\n",
    "\n",
    "dataset = TensorDataset(inputs, labels) \n",
    "dataloader = DataLoader(dataset)\n",
    "\n",
    "model = DecoderOnlyTransformer(num_tokens=len(token_to_id), d_model=2, max_len=6, seed=43)\n",
    "\n",
    "model_input = torch.tensor([token_to_id[\"what\"], \n",
    "                            token_to_id[\"is\"], \n",
    "                            token_to_id[\"DL\"], \n",
    "                            token_to_id[\"<EOS>\"]])\n",
    "\n",
    "def run_prompt(model_input: torch.tensor, model: L.LightningModule):\n",
    "    print(\">>> \", \" \".join([id_to_token[i.item()] for i in model_input]))\n",
    "\n",
    "    predictions = model(model_input) \n",
    "    predicted_id = torch.tensor([torch.argmax(predictions[-1,:])])\n",
    "    predicted_ids = predicted_id\n",
    "    input_length = model_input.size(dim=0)\n",
    "\n",
    "    max_length = 6\n",
    "    for i in range(input_length, max_length):\n",
    "        if (predicted_id == token_to_id[\"<EOS>\"]):\n",
    "            break\n",
    "\n",
    "        model_input = torch.cat((model_input, predicted_id))\n",
    "        \n",
    "        predictions = model(model_input) \n",
    "        predicted_id = torch.tensor([torch.argmax(predictions[-1,:])])\n",
    "        predicted_ids = torch.cat((predicted_ids, predicted_id))\n",
    "\n",
    "    print(\"Predicted Tokens:\\n\") \n",
    "    print(predicted_ids)\n",
    "    for id in predicted_ids: \n",
    "        print(\"\\t\", id_to_token[id.item()])\n",
    "\n",
    "def train_model(model: L.LightningModule, dataloader: DataLoader):\n",
    "    trainer = L.Trainer(max_epochs=30)\n",
    "    trainer.fit(model, train_dataloaders=dataloader)\n",
    "\n",
    "print(\"*** BEFORE TRAINING ***\")\n",
    "run_prompt(model_input, model)\n",
    "print(\"*** RUNNING TRAINING ***\")\n",
    "train_model(model, dataloader)\n",
    "print(\"*** AFTER TRAINING ***\")\n",
    "run_prompt(model_input, model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
